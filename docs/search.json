[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An Introduction to Machine Learning in R",
    "section": "",
    "text": "Session 1: Introducing Tidymodels (February 2023) - COMPLETED\nSession 2: Recipes and Workflows (February 2023) - COMPLETED\nSession 2: Modelling approaches (April 2023) - COMPLETED\n\nSupervised ML: 1) Random Forest, 2) SVM, 3) Xgboost\n\nSession 4: Cross-validation and Tuning (May 2023)\nSession 5: Model evaluation (June 2023)\n\nConfusion matrix metrics, ROC/AUC, SHAP\n\nSession 6: Troubleshooting your models (August 2023)\nSession 7: Presenting your models (September 2023)"
  },
  {
    "objectID": "index.html#introducing-tidymodels",
    "href": "index.html#introducing-tidymodels",
    "title": "An Introduction to Machine Learning in R",
    "section": "Introducing Tidymodels",
    "text": "Introducing Tidymodels\nWe introduced the xgboost package in R in our first session in December 2022. We got some predictions, feature importance, got the confusion matrix and extracted SHAP values. HOWEVER! You may have realised that for every package you called in- the code you had to write was slightly different and even worse you may have experienced some conflicting dependencies!\nTidymodels is an elegant solution to overcome this issue! It allows us to build our models using a standardised grammar but swap out the engine (modelling approach) as we like using the parsnip package.\n\n\n\nTidymodels at the heart of the Data Science Workflow (Source: R for Data Science, 2023)\n\n\nIt’s important to remember that the tidymodels package (just like the tidyverse package) compiles multiple different packages- utilising each for a specific task. Think of different packages acting like different parts of a race car or the process of baking a cake.\n\n\n\nWorkflow of packages utilised in Tidymodels (Source: R for Data Science, 2023)\n\n\n\nrsample - Different types of re-samples\nrecipes - Transformations for model data pre-processing\nparnip - A common interface for model creation\nyardstick - Measure model performance"
  },
  {
    "objectID": "index.html#recipes-and-workflows",
    "href": "index.html#recipes-and-workflows",
    "title": "An Introduction to Machine Learning in R",
    "section": "Recipes and Workflows",
    "text": "Recipes and Workflows\nLet’s start!\n\nlibrary(tidymodels)\nlibrary(readr)#read in csv\nlibrary(plotly)\nlibrary(kableExtra) #nicer tables in quarto\nlibrary(cowplot)#put several figures together\nlibrary(shapviz)\nlibrary(pkgdown)\n\n\nThe data\nTeam 3 (Kim and Jason) will be working on NHMS data from 2015 specifically looking at the rates of undiagnosed Type II Diabetes, Hypertension and Cholesterol. As the data is confidential- a dummy dataset was developed for the purposes of this tutorial by sampling from and scrambling up the original data. There are 5,000 rows of data provided- each row does being an individual. Please note as the data has been scrambled- EACH ROW IS NOT AN ACTUAL PERSONS RESPONSES. As such the results from this tutorial are also not valid. The data has also been cleaned and only variables of interest were selected.\n\nanalytic_df <- read_csv(\"data/nhms_dummy.csv\", \n    col_types = cols(sex = col_number(), \n        age = col_number(), age_cat = col_number(), \n        ethnic = col_number(), residential = col_number(), \n        married = col_number(), education = col_number(), \n        income = col_number(), hh_income = col_number(), \n        occupation = col_number(), cons_fv = col_number(), \n        alcohol = col_number(), smoke = col_number(), \n        pa = col_number(), undx_dm = col_number(), \n        undx_hpt = col_number(), undx_chol = col_number(), \n        hpt = col_number(), dm = col_number(), \n        chol = col_number(), bmi_cat = col_number(), \n        bmi = col_number(), waist = col_number(), \n        weight = col_number(), height = col_number(), \n        sys = col_number(), dys = col_number(), \n        rbs = col_number(), rchol = col_number(), \n        mean_f = col_number(), mean_v = col_number(), \n        pa_3cat = col_number(), vig_day = col_number(), \n        vig_min = col_number(), mod_day = col_number(), \n        mod_min = col_number(), walk_day = col_number(), \n        walk_min = col_number()))\n\n\n\nExploratory data analysis\nWe can just have a quick look at the data\n\nanalytic_df %>% \n  skimr::skim()\n\n\n\n\nData summary\n\n\n\n\nName\n\n\nPiped data\n\n\n\n\nNumber of rows\n\n\n5000\n\n\n\n\nNumber of columns\n\n\n38\n\n\n\n\n_______________________\n\n\n\n\n\n\nColumn type frequency:\n\n\n\n\n\n\nnumeric\n\n\n38\n\n\n\n\n________________________\n\n\n\n\n\n\nGroup variables\n\n\nNone\n\n\n\n\n\nVariable type: numeric\n\n\n\n\n\nskim_variable\n\n\nn_missing\n\n\ncomplete_rate\n\n\nmean\n\n\nsd\n\n\np0\n\n\np25\n\n\np50\n\n\np75\n\n\np100\n\n\nhist\n\n\n\n\n\n\nsex\n\n\n0\n\n\n1.00\n\n\n1.49\n\n\n0.50\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n2.00\n\n\n2.00\n\n\n▇▁▁▁▇\n\n\n\n\nage\n\n\n0\n\n\n1.00\n\n\n40.23\n\n\n15.71\n\n\n18.00\n\n\n27.00\n\n\n38.00\n\n\n51.00\n\n\n92.00\n\n\n▇▆▅▂▁\n\n\n\n\nage_cat\n\n\n0\n\n\n1.00\n\n\n3.11\n\n\n1.57\n\n\n1.00\n\n\n2.00\n\n\n3.00\n\n\n5.00\n\n\n5.00\n\n\n▆▅▃▃▇\n\n\n\n\nethnic\n\n\n0\n\n\n1.00\n\n\n3.97\n\n\n1.57\n\n\n1.00\n\n\n3.00\n\n\n5.00\n\n\n5.00\n\n\n5.00\n\n\n▂▁▁▁▇\n\n\n\n\nresidential\n\n\n0\n\n\n1.00\n\n\n1.58\n\n\n0.49\n\n\n1.00\n\n\n1.00\n\n\n2.00\n\n\n2.00\n\n\n2.00\n\n\n▆▁▁▁▇\n\n\n\n\nmarried\n\n\n0\n\n\n1.00\n\n\n1.60\n\n\n0.87\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n3.00\n\n\n3.00\n\n\n▇▁▁▁▃\n\n\n\n\neducation\n\n\n39\n\n\n0.99\n\n\n2.17\n\n\n0.79\n\n\n1.00\n\n\n2.00\n\n\n2.00\n\n\n3.00\n\n\n4.00\n\n\n▃▇▁▅▁\n\n\n\n\nincome\n\n\n0\n\n\n1.00\n\n\n2.47\n\n\n0.85\n\n\n1.00\n\n\n2.00\n\n\n3.00\n\n\n3.00\n\n\n3.00\n\n\n▃▁▁▁▇\n\n\n\n\nhh_income\n\n\n0\n\n\n1.00\n\n\n4121.92\n\n\n4587.07\n\n\n0.00\n\n\n1500.00\n\n\n2977.00\n\n\n5309.00\n\n\n63950.00\n\n\n▇▁▁▁▁\n\n\n\n\noccupation\n\n\n844\n\n\n0.83\n\n\n2.31\n\n\n1.40\n\n\n1.00\n\n\n1.00\n\n\n2.00\n\n\n3.00\n\n\n5.00\n\n\n▇▅▃▁▃\n\n\n\n\ncons_fv\n\n\n21\n\n\n1.00\n\n\n1.97\n\n\n0.16\n\n\n1.00\n\n\n2.00\n\n\n2.00\n\n\n2.00\n\n\n2.00\n\n\n▁▁▁▁▇\n\n\n\n\nalcohol\n\n\n2\n\n\n1.00\n\n\n1.90\n\n\n0.30\n\n\n1.00\n\n\n2.00\n\n\n2.00\n\n\n2.00\n\n\n2.00\n\n\n▁▁▁▁▇\n\n\n\n\nsmoke\n\n\n1\n\n\n1.00\n\n\n2.54\n\n\n0.84\n\n\n1.00\n\n\n3.00\n\n\n3.00\n\n\n3.00\n\n\n3.00\n\n\n▂▁▁▁▇\n\n\n\n\npa\n\n\n47\n\n\n0.99\n\n\n1.30\n\n\n0.46\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n2.00\n\n\n2.00\n\n\n▇▁▁▁▃\n\n\n\n\nundx_dm\n\n\n0\n\n\n1.00\n\n\n0.11\n\n\n0.32\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n▇▁▁▁▁\n\n\n\n\nundx_hpt\n\n\n0\n\n\n1.00\n\n\n0.23\n\n\n0.42\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n▇▁▁▁▂\n\n\n\n\nundx_chol\n\n\n0\n\n\n1.00\n\n\n0.46\n\n\n0.50\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n1.00\n\n\n1.00\n\n\n▇▁▁▁▇\n\n\n\n\nhpt\n\n\n0\n\n\n1.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n▁▁▇▁▁\n\n\n\n\ndm\n\n\n0\n\n\n1.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n▁▁▇▁▁\n\n\n\n\nchol\n\n\n0\n\n\n1.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n0.00\n\n\n▁▁▇▁▁\n\n\n\n\nbmi_cat\n\n\n382\n\n\n0.92\n\n\n1.64\n\n\n0.89\n\n\n1.00\n\n\n1.00\n\n\n1.00\n\n\n3.00\n\n\n3.00\n\n\n▇▁▁▁▃\n\n\n\n\nbmi\n\n\n382\n\n\n0.92\n\n\n25.36\n\n\n5.42\n\n\n12.82\n\n\n21.55\n\n\n24.58\n\n\n28.50\n\n\n55.33\n\n\n▃▇▂▁▁\n\n\n\n\nwaist\n\n\n387\n\n\n0.92\n\n\n84.46\n\n\n12.73\n\n\n35.00\n\n\n76.00\n\n\n84.00\n\n\n92.00\n\n\n149.00\n\n\n▁▆▇▁▁\n\n\n\n\nweight\n\n\n374\n\n\n0.93\n\n\n64.97\n\n\n15.39\n\n\n29.20\n\n\n54.00\n\n\n62.80\n\n\n73.80\n\n\n150.70\n\n\n▃▇▂▁▁\n\n\n\n\nheight\n\n\n367\n\n\n0.93\n\n\n159.91\n\n\n9.05\n\n\n124.60\n\n\n153.50\n\n\n159.80\n\n\n166.40\n\n\n196.00\n\n\n▁▃▇▃▁\n\n\n\n\nsys\n\n\n0\n\n\n1.00\n\n\n121.19\n\n\n33.92\n\n\n-9.00\n\n\n114.00\n\n\n123.50\n\n\n136.00\n\n\n250.00\n\n\n▁▁▇▁▁\n\n\n\n\ndys\n\n\n0\n\n\n1.00\n\n\n74.60\n\n\n21.38\n\n\n-9.00\n\n\n70.00\n\n\n77.50\n\n\n84.00\n\n\n169.00\n\n\n▁▁▇▁▁\n\n\n\n\nrbs\n\n\n5\n\n\n1.00\n\n\n4.29\n\n\n4.54\n\n\n-9.00\n\n\n4.20\n\n\n4.90\n\n\n5.80\n\n\n33.30\n\n\n▁▇▁▁▁\n\n\n\n\nrchol\n\n\n17\n\n\n1.00\n\n\n4.09\n\n\n4.10\n\n\n-9.00\n\n\n4.18\n\n\n5.07\n\n\n5.95\n\n\n10.30\n\n\n▁▁▁▇▂\n\n\n\n\nmean_f\n\n\n19\n\n\n1.00\n\n\n0.78\n\n\n0.83\n\n\n0.00\n\n\n0.29\n\n\n0.57\n\n\n1.00\n\n\n23.00\n\n\n▇▁▁▁▁\n\n\n\n\nmean_v\n\n\n9\n\n\n1.00\n\n\n1.42\n\n\n1.19\n\n\n0.00\n\n\n0.86\n\n\n1.00\n\n\n2.00\n\n\n19.00\n\n\n▇▁▁▁▁\n\n\n\n\npa_3cat\n\n\n47\n\n\n0.99\n\n\n2.03\n\n\n0.76\n\n\n1.00\n\n\n1.00\n\n\n2.00\n\n\n3.00\n\n\n3.00\n\n\n▅▁▇▁▆\n\n\n\n\nvig_day\n\n\n34\n\n\n0.99\n\n\n2.10\n\n\n2.66\n\n\n-7.00\n\n\n0.00\n\n\n0.00\n\n\n4.00\n\n\n7.00\n\n\n▁▁▇▂▃\n\n\n\n\nvig_min\n\n\n34\n\n\n0.99\n\n\n38.65\n\n\n84.42\n\n\n-7.00\n\n\n0.00\n\n\n0.00\n\n\n30.00\n\n\n720.00\n\n\n▇▁▁▁▁\n\n\n\n\nmod_day\n\n\n34\n\n\n0.99\n\n\n4.41\n\n\n2.65\n\n\n0.00\n\n\n2.00\n\n\n5.00\n\n\n7.00\n\n\n7.00\n\n\n▃▂▃▂▇\n\n\n\n\nmod_min\n\n\n34\n\n\n0.99\n\n\n41.06\n\n\n63.18\n\n\n-9.00\n\n\n10.00\n\n\n30.00\n\n\n45.00\n\n\n780.00\n\n\n▇▁▁▁▁\n\n\n\n\nwalk_day\n\n\n34\n\n\n0.99\n\n\n4.02\n\n\n2.94\n\n\n-7.00\n\n\n0.00\n\n\n5.00\n\n\n7.00\n\n\n7.00\n\n\n▁▁▅▃▇\n\n\n\n\nwalk_min\n\n\n34\n\n\n0.99\n\n\n23.50\n\n\n45.85\n\n\n-7.00\n\n\n0.00\n\n\n15.00\n\n\n30.00\n\n\n720.00\n\n\n▇▁▁▁▁\n\n\n\n\n\n\n\nWe can also plot anything of interest. With bigger or big data this is usually a good idea! Let look at the differences in bmi and random blood sugar by income status\n\np <- ggplot(analytic_df,\n       aes(x = bmi, \n           y = rbs, \n           group = factor(income), \n           col = factor(income))) + \n  geom_point(alpha=0.2) + \n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)\nggplotly(p)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nLets try another configuration- this time instead of blood sugar lets look at cholestrol\n\np <- ggplot(analytic_df,\n       aes(x = bmi, \n           y = rchol, \n           group = factor(income), \n           col = factor(income))) + \n  geom_point(alpha=0.2) + \n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)\nggplotly(p)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\nor something else- lets try systolic BP\n\np <- ggplot(analytic_df,\n       aes(x = bmi, \n           y = sys, \n           group = factor(income), \n           col = factor(income))) +\n  geom_point(alpha=0.2) + \n  geom_smooth(method = lm, se = FALSE) +\n  scale_color_viridis_d(option = \"plasma\", end = .7)\nggplotly(p)\n\n`geom_smooth()` using formula 'y ~ x'\n\n\n\n\n\n\n\n\nRecipes for Pre-processing\n\nRecipes\nIn practice, it’s common to perform some preprocessing of the data to make it easier for the algorithm to fit a model to it. There’s a huge range of preprocessing transformations you can perform to get your data ready for modeling, but we’ll limit ourselves to a few common techniques:\n\nScaling numeric features so they’re on the same scale. This prevents features with large values from producing coefficients that disproportionately affect the predictions.\nEncoding categorical variables. For example, by using a one hot encoding technique you can create “dummy” or indicator variables which replace the original categorical feature with numeric columns whose values are either 1 or 0.\n\nFor our example, lets focus on the prediction of Undiagnosed DM. Lets select some key predictors a priori just to make this tutorial a little easier. We will work on age, sex, income status, residential area, smoker, bmi, systolic bp, mean fruit and vegetable intake, vigourous activity (day and minutes), systolic blood pressure, random blood sugar and random cholestrol levels. Please note that this data has already been cleaned and coded. For the sake of the example we will have a mix of character and numeric variables.\n\ndf <- analytic_df %>% \n  select(undx_dm, age, sex, residential, income, \n         smoke, bmi, mean_f, mean_v,\n         vig_day, vig_min, sys, rbs, rchol) %>%\n  mutate(undx_dm=recode(undx_dm, `0`=\"No\", `1`=\"Yes\"),\n         sex=recode(sex, `1`=\"Female\", `2`=\"Male\"),\n         residential=recode(residential, `1`=\"Rural\", `2`=\"Urban\"),\n         income=recode(income, `1`=\"M40\", `2`=\"T20\", `3`=\"B40\"),\n         smoke=recode(smoke, `1`=\"Current\", `2`=\"Former\", `3`=\"Never\")) \n\n\n\nTrain-test split\nresample is a core package in tidymodels. It provides a streamlined way to create a randomised training and test split of the original data. If we want to achieve an 80:20 split the inputs to the resample function are straightforward: data = df, prop = 0.80. We also set a seed so we can reproduce the results. With that we get an 80:20 split. That easy.\n\nset.seed(seed = 4763) \n\ntrain_test_split <-\n  rsample::initial_split(\n    data = df,     \n    prop = 0.80   \n  ) \ntrain_test_split\n\n<Training/Testing/Total>\n<4000/1000/5000>\n\n\n\ntrain_tbl <- train_test_split %>% training() \ntest_tbl  <- train_test_split %>% testing()\n\n\n\nTransforms, One-hot encoding and imputation\nThe recipes package uses a cooking metaphor for data preprocessing - missing values, imputation, centering and scaling and one-hot-encoding. Lets first try and get a summary of the structure within the data.\n\nrec <- recipe( ~ ., data = train_tbl)\nsummary(rec)\n\n# A tibble: 14 × 4\n   variable    type    role      source  \n   <chr>       <chr>   <chr>     <chr>   \n 1 undx_dm     nominal predictor original\n 2 age         numeric predictor original\n 3 sex         nominal predictor original\n 4 residential nominal predictor original\n 5 income      nominal predictor original\n 6 smoke       nominal predictor original\n 7 bmi         numeric predictor original\n 8 mean_f      numeric predictor original\n 9 mean_v      numeric predictor original\n10 vig_day     numeric predictor original\n11 vig_min     numeric predictor original\n12 sys         numeric predictor original\n13 rbs         numeric predictor original\n14 rchol       numeric predictor original\n\n\nIt is obvious from the output there are several nominal variables (character). We knew this because we had already converted them back earlier. We can now proceed to use a simple step to get them into factors as ML models in tidymodels use factors (for categorical vars).\n\nrecipe_simple <- function(dataset) {\n  recipe(undx_dm ~ ., data = dataset) %>%\n    step_string2factor(all_nominal(), -all_outcomes()) %>%\n    prep(data = dataset)\n}\n\ndf_factorised <- recipe_simple(dataset= train_tbl)\nsummary(df_factorised)\n\n# A tibble: 14 × 4\n   variable    type    role      source  \n   <chr>       <chr>   <chr>     <chr>   \n 1 age         numeric predictor original\n 2 sex         nominal predictor original\n 3 residential nominal predictor original\n 4 income      nominal predictor original\n 5 smoke       nominal predictor original\n 6 bmi         numeric predictor original\n 7 mean_f      numeric predictor original\n 8 mean_v      numeric predictor original\n 9 vig_day     numeric predictor original\n10 vig_min     numeric predictor original\n11 sys         numeric predictor original\n12 rbs         numeric predictor original\n13 rchol       numeric predictor original\n14 undx_dm     nominal outcome   original\n\n\nFinally to improve model performance we carry out one-hot encoding (dummy variables) although some if not most newer implementations no longer require this step. If you remember income and smoking status have >2 levels.\n\ndf_factorised_coded = df_factorised %>% \n  step_dummy(c(income, smoke), one_hot = T) %>%\n  prep(training=train_tbl, retain=T)\n\nLets check the data again\n\nsummary(df_factorised_coded)\n\n# A tibble: 18 × 4\n   variable      type    role      source  \n   <chr>         <chr>   <chr>     <chr>   \n 1 age           numeric predictor original\n 2 sex           nominal predictor original\n 3 residential   nominal predictor original\n 4 bmi           numeric predictor original\n 5 mean_f        numeric predictor original\n 6 mean_v        numeric predictor original\n 7 vig_day       numeric predictor original\n 8 vig_min       numeric predictor original\n 9 sys           numeric predictor original\n10 rbs           numeric predictor original\n11 rchol         numeric predictor original\n12 undx_dm       nominal outcome   original\n13 income_B40    numeric predictor derived \n14 income_M40    numeric predictor derived \n15 income_T20    numeric predictor derived \n16 smoke_Current numeric predictor derived \n17 smoke_Former  numeric predictor derived \n18 smoke_Never   numeric predictor derived \n\n\nUnfortunately we have some missing variables. Lets try imputing the data using a linear model. This may not be the best approach but works great for an example\n\ndf_factorised_coded_imp = df_factorised_coded %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  prep(training=train_tbl, retain=T)\n\nEverything looks good. Final step is to bake all the steps into the data\n\ntrain_baked <- bake(df_factorised_coded_imp, new_data = train_tbl)\ntest_baked  <- bake(df_factorised_coded_imp, new_data = test_tbl)\n\n\n\n\nWorkflows for model building\ntidymodels leans on the parsnip package for its model building. parsnip offers a unified API that allows access to a variety of analytic packages without the requirement of learning the syntax for each package. It only takes three simple steps to fit models:\n\nPick the type of Model - Lets start with the most basic - logistic regression\nSpecify the engine - we’ll use glm\nDefine the model specification / formula and data - We will saturate the model with all our selected variables\n\nIn Tidymodels, this convenient object is called a workflow and conveniently holds your modeling components. The workflows package allows the user to bind modeling and preprocessing objects together. You can then fit the entire workflow to the data, such that the model encapsulates all of the preprocessing steps as well as the algorithm.\n\n#define a recipe\nlogistic_recipe <- recipe(undx_dm~., data=train_baked)\n\n#specify the model\nlogistic_spec <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  set_mode(\"classification\")\n\n#define a workflow\nlogistic_workflow<- workflow() %>% \n  add_recipe(logistic_recipe) %>% \n  add_model(logistic_spec)\n\nlogistic_glm <- logistic_workflow %>% \n  fit(data = train_baked)\n\nThis is a sample of a workflow. Ideally you would also integrate the above steps into the workflow like this:\n\n#define a recipe\nlogistic_recipe <- recipe(undx_dm~., data=train_tbl) %>%\n  step_novel(all_predictors(), -all_numeric()) %>% \n  step_string2factor(all_nominal(), -all_outcomes()) %>%\n  step_dummy(income, one_hot = T) %>%\n  step_dummy(smoke, one_hot = T) %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  step_impute_mean(smoke_Never, smoke_Former, smoke_Current) %>%\n  step_zv(all_predictors()) \n\n#specify the model\nlogistic_spec <- logistic_reg() %>% \n  set_engine(\"glm\") %>% \n  set_mode(\"classification\")\n\n#define a workflow\nlogistic_workflow<- workflow() %>% \n  add_recipe(logistic_recipe) %>% \n  add_model(logistic_spec)\n\n#run model\nlogistic_glm <- logistic_workflow %>% \n  fit(data = train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n\nIf we wanted to switch to a different engine, all we would have to do change the set_engine argument to the desired tool and parsnip handles all dirty work behind the scenes - I said unified API!\n\nset_engine(“glmnet”)\nset_engine(“lm”)\nset_engine(“spark”)\nset_engine(“keras”)\n\nThere’s a long list of engines we can use. Check out the tidymodels page for more details on engines and models.\n\n\nA note on the Bias-Variance tradeoff in ML\n\n\n\nBias-Variance tradeoff in ML (Source: Tassopoulou, 2019 & Kaggle blog, 2020)\n\n\nA model with high bias (underfitting) is one that is too simple to capture the complexity of the underlying data. For example, a linear regression model might be too simple to capture the non-linear relationship between the features and the target variable. This can result in poor performance on both the training and test data, as the model is not able to capture the true relationship between the features and the target variable.\nA model with high variance (overfitting) is one that is too complex and fits the training data too closely, but does not generalize well to new data. For example, a decision tree model with many branches might fit the training data very well, but may not be able to generalize to new data that is different from the training data. This can result in good performance on the training data, but poor performance on the test data."
  },
  {
    "objectID": "index.html#supervised-learning-for-classification-tasks",
    "href": "index.html#supervised-learning-for-classification-tasks",
    "title": "An Introduction to Machine Learning in R",
    "section": "Supervised learning for Classification tasks",
    "text": "Supervised learning for Classification tasks\nThe above example was a simple example using a simple example using Logistic regression for a binary classification task- but with ML we can expand into comparing different models for the same task.\nThe goal of Supervised classification: “Learn a classification model (classifier) from the labeled instances so as to predict labels for (classify) unseen instances”\nIn other words the model learns from a training set (classifier of labelled instances) to predict on test set (classifier of unseen instances)\nTypes of supervised models include: Decision trees, k-Nearest Neighbours, Support Vector Machines, Naive Bayes, Logistic Regression, Bayesian networks/ Neural networks, Genetic Algorithms, Ensemble methods: Bagging and Boosting etc.\nIn this course we will focus on three frequently utilised methods:\n1) Decision trees with implementation of Random Forest\n2) Support Vector Machines\n3) Gradient Boosting\n\nDecision Trees\n\nTheory\nA decision tree is a way to make decisions using a tree-like structure. Each node in the tree represents a question, and each branch represents a possible answer to that question. The tree starts at the top with a question, and depending on the answer, it moves down to the next level of the tree and asks another question. This continues until the tree reaches a leaf node, which represents a final decision or outcome.\nWhen a decision tree classifies things into categories its called a Classification Tree (like image below)\nWhen a decision tree predicts numeric values its called a Regression Tree.\nLets see how classification trees are built:\n\n\n\nA very Malaysian decision tree (Addapted from StatQuest, Youtbe 2019)\n\n\nEach part of the tree has its own definitions\n\n\n\nSome definitions (Adapted from StatQuest, Youtube 2019)\n\n\nBut how can we decide where does the root node start? Well we can use several different mathematical solutions that include the Gini Impurity, Entropy, Information Gain etc. They sound tough but mathematically are very similar. Lets start with an example of Gini Impurity\n\n\n\nDummy data on Prediciting Samyang love (Adapted from StatQuest, Youtube 2019)\n\n\n\n\n\nCalculating Gini Impurity for Sex variable (Adapted from StatQuest, Youtube 2019)\n\n\n\n\n\nCalculating Gini Impurity for Loves Blackpink variable (Adapted from StatQuest, Youtube 2019)\n\n\nFor continuous variables like age above, its a little more work but is still very simple. We do this by first iteratively building through different thresholds in the data by taking and average of two consequent rows and using this threshold to calculate the Gini. We can do this for each row and find the lowest impurity and select that threshold as the one to be used in the model.\nOnce we select a leaf to start we can keep adding leaves by iteratively repeating the step above by carrying out another Gini at each level. We do this until there are no more values to split. And whatever we have left we can label these as our outcomes and predict on these.\n\n\n\nAdding leaves and labelling outputs (Adapted from StatQuest, Youtube 2019)\n\n\nNow you may a question about regression trees- and the concept is just as simple- regression trees are use when the outcomes to be predicted for is continuous and instead of using a Gini Impurity (or entropy etc) we a sum of square residuals for multiple thresholds of the data just like in for the continuous example above.\nONE IMPORTANT CAVEAT TO DECISION TREES!!\nIf we look at the use of the age threshold <12.5 from the continuous sample above, only 1 sample fits from the data suggesting that we could have potentially OVERFIT our model (i.e. we can have very little confidence in data that is predicted in this way). There are two methods to overcome this 1) Pruning, and 2) Limiting how trees grow. For pruning it will be covered sometime later (maybe). For limiting how trees grow we will cover these during the random forest specification later and also when we later learn to tune our models with Cross Validation.\n\n\nRandom Forest\nThe overfitting is the perfect introduction to Random Forests.\n“Trees have one aspect that prevents them from being ideal the ideal tool for predictive learning,namely inaccuracy” - The Elements of Statistical Learning\nRandom forest uses iterative resampling and multiple trees to create flexibility\n1) Create a bootstrapped dataset- random\n2) Create a decision tree using the bootstrapped set using random subset of data at each step\nRepeat step 1) and 2) multiple times (100s) creating a variety of trees (i.e. a forest of random trees)\nAfter his run the prediction set (test set) through each and every tree- and check the number of trees that classify the outcome the same way- eg loves samyang yes5, no1.\nBagging- Bootstrapping + using the aggregate to make a decision is called “Bagging”\nTypically bootstrapping with replacement means 1/3rd of data will not be used. Out of bag dataset- the 1/3 rd that werent used- use this as a validation set the proportion of “out-of-bag” samples classified incorrectly is the “Out-of-bag error”.\nWe then go back and determine how many variables we use in step 2) eg we first use 2 random vars, then 3, 4 and so on and compare the accuracies of each forest.\n\n\n\nSimplified Schematization of Random Trees (Source: Catalyst Earth, 2023)\n\n\n\n\nBuilding a random forest classifier!\nAll three parameters work on the principle of the bias-variance tradeoff which is critical to ML. Reduce bias to much and you have an overfit model (highly accurate, computationally intensive, low intepretability), fit too simple a model and and it loses value (low accuracy, computationally simple, high intepretability). The three parameters in random forest are as below:\n\nmtry: This parameter determines the number of variables that are considered at each split when constructing a decision tree in the random forest model. In other words, it specifies the size of the random subset of features that is used to make each split. A larger value of mtry will result in more diverse trees, which may improve the accuracy of the model, but may also increase the computational cost and reduce the interpretability of the model. One common heuristic is to set mtry to the square root of the number of predictor variables. This means that, by default, approximately 30% of the variables are considered for each split.\ntrees: This parameter specifies the number of trees that are grown in the random forest model. A larger value of trees will generally result in better performance, but will also increase the computational cost and may lead to overfitting if the model is too complex for the amount of data available. A common heuristic is to use a large number of trees, such as 500 or 1000, to ensure that the model is robust and stable.\nmin_n: This parameter specifies the minimum number of samples that are required to split a node in a decision tree. A larger value of min_n will result in simpler trees, which may improve the interpretability of the model, but may also reduce the accuracy and increase the bias of the model if the trees are too simple for the complexity of the data. A common heuristic is to set min_n to a small value, such as 5, which allows the tree to grow deeper and capture more complex relationships in the data.\nP/S: Heuristics are just guidelines and may not be the best solution for your data/question.\n\n\n\nTutorial 1:\nYou have 5 minutes. Take into consideration: accuracy, computation, and intepretability.\nThere is a heuristic model, and three other models. Lets see who gets the best model in the alotted time.\n\n#define a recipe\nrf_recipe <- recipe(undx_dm~., data=train_tbl) %>%\n  step_novel(all_predictors(), -all_numeric()) %>% \n  step_string2factor(all_nominal(), -all_outcomes()) %>%\n  step_dummy(income, one_hot = T) %>%\n  step_dummy(smoke, one_hot = T) %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  step_impute_mean(smoke_Never, smoke_Former, smoke_Current) %>%\n  step_zv(all_predictors())\n\n#Heurisitics model\nrf_spec <- \n  rand_forest(mtry = 4,\n              trees = 500,\n              min_n = 5,\n              mode=\"classification\") %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\nrf_workflow<- workflow() %>% \n  add_recipe(rf_recipe) %>% \n  add_model(rf_spec)\nrf_heuristic <- rf_workflow %>% \n fit(data = train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#Model 1\nrf_spec <- \n  rand_forest(mtry = 6, #TUNE HERE\n              trees = 750, #TUNE HERE\n              min_n = 7,#TUNE HERE\n              mode=\"classification\") %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\nrf_workflow<- workflow() %>% \n  add_recipe(rf_recipe) %>% \n  add_model(rf_spec)\nrf_model1 <- rf_workflow %>% \n fit(data = train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#Model 2\nrf_spec <- \n  rand_forest(mtry = 8, #TUNE HERE\n              trees = 1000, #TUNE HERE\n              min_n = 9,#TUNE HERE\n              mode=\"classification\") %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\nrf_workflow<- workflow() %>% \n  add_recipe(rf_recipe) %>% \n  add_model(rf_spec)\nrf_model2 <- rf_workflow %>% \n fit(data = train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#Model 3\nrf_spec <- \n  rand_forest(mtry = 12, #TUNE HERE\n              trees = 10000, #TUNE HERE\n              min_n = 5, #TUNE HERE\n              mode=\"classification\") %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\nrf_workflow<- workflow() %>% \n  add_recipe(rf_recipe) %>% \n  add_model(rf_spec)\nrf_model3 <- rf_workflow %>% \n fit(data = train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#check accuracy of models\npredictions_rf_heu <- rf_heuristic %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\npredictions_rf_m1 <- rf_model1 %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\npredictions_rf_m2 <- rf_model2 %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\npredictions_rf_m3 <- rf_model3 %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\n\n#define metrics\neval_metrics <- metric_set(accuracy, ppv, recall, specificity, f_meas)\n#call metrics on models\neval_metrics_rf_heu <- eval_metrics(data = predictions_rf_heu, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Random Forest Heuristic Model\"=\".estimate\")\neval_metrics_rf_m1 <- eval_metrics(data = predictions_rf_m1, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Random Forest Model 1\"=\".estimate\")\neval_metrics_rf_m2 <- eval_metrics(data = predictions_rf_m2, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Random Forest Model 2\"=\".estimate\")\neval_metrics_rf_m3 <- eval_metrics(data = predictions_rf_m3, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Random Forest Model 3\"=\".estimate\")\n\n#put them together\nleft_join(eval_metrics_rf_heu, eval_metrics_rf_m1, by=\".metric\") %>%\n  left_join(eval_metrics_rf_m2, by=\".metric\") %>%\n  left_join(eval_metrics_rf_m3, by=\".metric\") %>%\n  kable()\n\n\n\n \n  \n    .metric \n    Random Forest Heuristic Model \n    Random Forest Model 1 \n    Random Forest Model 2 \n    Random Forest Model 3 \n  \n \n\n  \n    accuracy \n    0.9010000 \n    0.9010000 \n    0.9010000 \n    0.9010000 \n  \n  \n    ppv \n    0.9010000 \n    0.9010000 \n    0.9010000 \n    0.9010000 \n  \n  \n    recall \n    1.0000000 \n    1.0000000 \n    1.0000000 \n    1.0000000 \n  \n  \n    specificity \n    0.0000000 \n    0.0000000 \n    0.0000000 \n    0.0000000 \n  \n  \n    f_meas \n    0.9479221 \n    0.9479221 \n    0.9479221 \n    0.9479221 \n  \n\n\n\n\n\nWhat were your findings?\n\n\n\nSupport Vector Machines (SVM)\n\nTheory\nLets start by discussing a very simple example of classifying a single variable. Lets call it the spiciness of a meal (Spicy vs Not Spicy). The following example would be a good way to classify, no?\n\n\n\nThreshold Classification (Adapted from StatQuest, Youtube 2019)\n\n\nThreshold such as the above are prone to inaccuracies. A point slightly beyond the threshold will be classified as spicy although it looks more likely to be not spicy.\n\n\n\nMaximal margin classification (Adapted from StatQuest, Youtube 2019)\n\n\nMaximal margin classification is a great way to fix this problem by using the midpoint between edge cases. But what could possibly be problematic with this?\n\n\n\nEffect of the outlier (Adapted from StatQuest, Youtube 2019)\n\n\nMaximal margin classification is heavily effected by outliers as can be seen above. To overcome this we use something called soft margins- where we except some degree of misclassification. That degree is determined iteratively (cross validation).\n\n\n\nSoft Margins (Adapted from StatQuest, Youtube 2019)\n\n\nSoft margins (allowing misclassification) should produce a much more robust model- one that balance bias and variance. Soft margin classifier are also known as…??\nSUPPORT VECTOR CLASSIFIERS\n\n\n\nSoft margins in two dimensional space (Adapted from StatQuest, Youtube 2019)\n\n\nIn two dimensional space, if you imagine the above to be spiciness level by age you could then use a soft margin to classify as above.\n\n\n\nSoft margins in three dimensional space (Adapted from StatQuest, Youtube 2019)\n\n\nIf the same is carried in three dimensions we could use a plane to classify different groups. Mathematically this is called a hyperplane and mathematically all flat affine subspaces are considered hyperplanes and as such hyperplanes are used for all dimension classifiers even though we generally use the term only when we cannot draw the classifier on paper.\nBUT!\n\n\n\nTraining Data overlap (Adapted from StatQuest, Youtube 2019)\n\n\nWhen training data overlap- soft margins no longer function as well. Which brings us to our second intuition in SVMs. The idea of the polynomial kernel.\nA polynomial kernel draws a curved boundary that can separate data points that are not easily separated by a straight line. The degree of the polynomial determines how curved the boundary is. So, if you have a high-degree polynomial, the boundary can have lots of curves and bends, making it very flexible. This can be useful if your data is complex and requires a more complicated boundary.\nMain idea in SVM is:\n1) Start with data in relatively low dimensions\n2) Move the data into a higher dimension\n3) Find a support vector classifier that separates the higher dimensional data into 2 classes\nSome additional info:\nBesides the Polynomial kernel we can also use a radial kernel (RBF).\nAn RBF kernel draws a boundary that is more like a circle or a sphere. The kernel calculates the similarity between pairs of data points and puts them into groups based on their similarity. The center of each group is called a support vector. The kernel then draws a boundary around each support vector, which separates the data points that are similar to it from those that are not. This method can be useful when the data points are not easily separable by a straight line or a curved boundary.\n\n\n\nA Radial Basis Function Kernel (Source: Zhou 2017)\n\n\n\n\nBuilding an SVM Classifier\nJust like with random forest we can specify an SVM model in R. Using a polynomial kernel we can specify the model using parameters as follows:\n\ncost: This is a parameter that controls the trade-off between maximizing the margin and minimizing the classification error. A larger cost value leads to a smaller margin and a higher penalty for misclassifications, which can result in a more complex decision boundary that fits the training data better but may overfit. A smaller cost value leads to a larger margin and a lower penalty for misclassifications, which can result in a simpler decision boundary that is more generalizable but may underfit. A common heuristic for selecting the cost value is to start small. This parameters should however use a grid search or cross-validation to evaluate a range of values and choose the one that gives the best performance on a validation set.\ndegree: This is a parameter that controls the degree of the polynomial used to compute the decision boundary. A higher degree value means that the decision boundary can be more complex and have more bends and curves, which can result in better accuracy on the training data. However, a higher degree can also lead to overfitting, especially if the dataset is noisy or contains outliers. A common heuristic for selecting the degree value is to start with a low value and gradually increase it until the performance on a validation set stops improving.\n\nP/S: Heuristics are just guidelines and may not be the best solution for your data/question.\n\n\nTutorial 2: Comparing kernels\nAs SVMs take longer to compute- we shall not attempt the 4 model trial as we did in the random forest. Instead lets try in the next 5 minutes to compare a polynomial kernel to a RBF kernel. The only difference will be the specification which replaces degree for gamma.\n\nrbf_sigma: This is the gamma parameter in the RBF kernel, which controls the width of the kernel function. A small value of rbf_sigmameans that the kernel function has a large radius and will consider many points when computing the decision boundary. This can lead to overfitting, especially if the dataset is noisy or contains outliers. A large value of rbf_sigmameans that the kernel function has a small radius and will only consider nearby points when computing the decision boundary. This can lead to underfitting, especially if the dataset is complex or has a large number of features. The “scale” option for rbf_sigmais a common choice, which scales rbf_sigmaby the inverse of the number of features in the dataset.\n\nChange the parameters below as you see fit:\n\n#define a recipe\nsvm_recipe <- recipe(undx_dm~., data=train_tbl) %>%\n  step_novel(all_predictors(), -all_numeric()) %>% \n  step_string2factor(all_nominal(), -all_outcomes()) %>%\n  step_dummy(income, one_hot = T) %>%\n  step_dummy(smoke, one_hot = T) %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  step_impute_mean(smoke_Never, smoke_Former, smoke_Current) %>%\n  step_mutate(sex=as.numeric(sex),\n         residential=as.numeric(sex)) %>%\n  step_zv(all_predictors())\n\n#Polynomial model\n# Define the tuning specification\nsvm_spec <- svm_poly(\n  cost = 0.8, #TUNE HERE\n  degree = 3) %>%#TUNE HERE\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n\n# Workflow for tuning\nsvm_workflow <- \n  workflow() %>%\n  add_recipe(svm_recipe) %>%\n  # add the tuning specificiatons\n  add_model(svm_spec)\n\n# Start Tuning\nsvm_model_poly <-  svm_workflow %>%\n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#RBF model\n# Define the tuning specification\nsvm_spec <- svm_rbf(\n  cost = 0.8, #TUNE HERE\n  rbf_sigma  = 0.5) %>% #TUNE HERE\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n\n# Workflow for tuning\nsvm_workflow <- \n  workflow() %>%\n  add_recipe(svm_recipe) %>%\n  # add the tuning specificiatons\n  add_model(svm_spec)\n\n# Start Tuning\nsvm_model_rbf <-  svm_workflow %>%\n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#check accuracy of models\npredictions_svm_poly <- svm_model_poly %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\npredictions_svm_rbf <- svm_model_rbf %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\n\n#define metrics\neval_metrics <- metric_set(accuracy, ppv, recall, specificity, f_meas)\n#call metrics on models\neval_metrics_svm_poly <- eval_metrics(data = predictions_svm_poly, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"SVM Polynomial\"=\".estimate\")\neval_metrics_svm_rbf <- eval_metrics(data = predictions_svm_rbf, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"SVM RBF\"=\".estimate\")\n\n#put them together\nleft_join(eval_metrics_svm_poly, eval_metrics_svm_rbf, by=\".metric\") %>%\n  kable()\n\n\n\n \n  \n    .metric \n    SVM Polynomial \n    SVM RBF \n  \n \n\n  \n    accuracy \n    0.8680000 \n    0.9010000 \n  \n  \n    ppv \n    0.9009385 \n    0.9010000 \n  \n  \n    recall \n    0.9589345 \n    1.0000000 \n  \n  \n    specificity \n    0.0404040 \n    0.0000000 \n  \n  \n    f_meas \n    0.9290323 \n    0.9479221 \n  \n\n\n\n\n\n\n\n\nExtreme gradient boosting\n\nTheory\n\nGradient Boosting\nFirstly, xgboost is developed from the concept of Gradient boosting! The main idea behind this algorithm is to build models sequentially and these subsequent models try to reduce the errors of the previous model (boosting). When the predictor is continuous, we use Gradient Boosting Regressor whereas when it is a classification problem, we use Gradient Boosting Classifier. The only difference between the two is the Loss function. The objective here is to minimize this loss function by adding weak learners using gradient descent. Since it is based on loss function hence for regression problems, we’ll have different loss functions like Mean squared error (MSE) and for classification, we will have different for e.g log-likelihood.\nSteps to fit a gradient boosting model:\n\nBuild a leaf- usually the average of the predictor (if continuous).\nInitialize the model with a simple algorithm, such as a decision tree with a small depth (between 8-32 leaves), and fit it to the training data.\nCalculate the residual errors of the model by subtracting the predicted values from the true values.\nFit a new model to the residual errors, which will predict the difference between the true values and the predictions of the previous model.\nUpdate the predictions of the previous model by adding the predictions of the new model, weighted by a learning rate (Scaling variable), to create a new and improved set of predictions.\nScale the tree and move on!\nRepeat steps 2-4 until the desired level of accuracy is achieved or a predefined number of models have been added to the ensemble.\n\n\n\n\nGradient Boosting Algorithm (Adapted from StatQuest, Youtube 2019)\n\n\n\n\nextremeGradient Boosting (xgboost)\nSo how is Xgboost different from plain old gradient boosting:\n\nXgboost trees: Xgboost uses a unique decision tree as opposed to a regular decision tree in gradient boosting. It uses the gain statistics (as opposed to impurity if you still remember).\nFirst we calculate a Similarity score for a given threshold= Sum of residuals square/ number of residuals + lambda (regularization parameter). Calculate a total similarity, and for each leaf. Then estimate the Gain=Left leaf similarity + Right leaf similarity - Root similarity. This is how xgboost determine how to classify.\nRegularization: Xgboost uses a more regularized model formulation to prevent overfitting. It does this by adding a regularization term (lambda) to the objective function that penalizes model complexity. This helps to reduce the variance of the model and prevent overfitting. Xgboost utilises a pruning parameter gamma, based on gai to determine if a split should be removed. (Gain-gamma=+ value (keep) or -value (remove)\nApproximate greedy algorithm: The XGBoost model uses an approximate greedy algorithm to build the decision trees. The algorithm is designed to be fast and scalable, making it suitable for large datasets. During the tree construction process, the algorithm makes use of a weighted quantile sketch to efficiently calculate the approximate quantiles of the input features. This helps to reduce the number of splits that need to be evaluated, which in turn speeds up the tree construction process.\nWeighted quantile sketch: The weighted quantile sketch is a data structure that is used to calculate the approximate quantiles of the input features. It works by assigning weights to the input features based on their importance, and then computing the approximate quantiles using these weighted values. This allows the XGBoost model to efficiently split the data at each node of the decision tree, which is important for creating accurate and efficient models. Here’s an example of how the weighted quantile sketch works:\nSuppose we have a dataset with a single feature, and we want to find the median value of that feature. The traditional approach would be to sort the data and find the middle value. However, this approach can be slow and memory-intensive, especially for large datasets. With the weighted quantile sketch, we can approximate the median value using a much smaller number of calculations. The sketch assigns weights to each data point based on its position in the sorted list, and then calculates the weighted median using these values.\nSparsity-aware split finding: XGBoost is designed to work with sparse data, which is common in many real-world applications. The sparsity-aware split finding algorithm used in XGBoost is designed to handle missing values and efficiently split the data based on the non-zero values of the input features. This allows the model to effectively handle sparse data without sacrificing performance. Here’s an example of how sparsity-aware split finding works:\nSuppose we have a dataset with many missing values. The traditional approach would be to impute these missing values, which can be time-consuming and may introduce bias into the model. With sparsity-aware split finding, XGBoost can effectively handle missing values without imputation. The algorithm identifies the non-zero values of the input features and uses these values to split the data. This allows the model to effectively use the available data without relying on imputation.\nParallel learning: XGBoost can be trained in parallel on multiple processors or machines, which speeds up the training process. The model uses a data parallelism approach, where each worker processes a subset of the data independently and then aggregates the results. This allows XGBoost to effectively scale to large datasets and compute resources. Here’s an example of how parallel learning works:\nSuppose we have a large dataset that we want to train a model on. With traditional machine learning algorithms, training on this dataset could take a long time and may require a large amount of memory. With parallel learning in XGBoost, we can split the dataset into smaller subsets and train the model on each subset in parallel. This allows us to effectively use multiple processors or machines and greatly speed up the training process.\nCache-aware access: XGBoost is designed to minimize the amount of data movement between memory and cache, which can be a bottleneck in many machine learning algorithms. The model uses a cache-aware access strategy that ensures that the most frequently accessed data is stored in cache, which reduces the amount of data movement and speeds up the training process. Here’s an example of how cache-aware access works:\nSuppose we have a dataset that is too large to fit into memory. With traditional machine learning algorithms, loading data from disk can be slow\n\n\n\nA note on Gradient descent\n\n\n\nGradient Descent (Source: Hikmat et al, 2021)\n\n\nGradient descent is used to find the minimum point of a mathematical function. The function might have many peaks and valleys, and we want to find the lowest valley. We start at some point and take steps in the direction of the steepest descent (or negative gradient) until we reach the lowest point.\n\n\nA note on regularization\nL1 regularization tries to simplify a model by setting some of its coefficients (or weights) to zero. This helps prevent overfitting, which is when a model becomes too complex and fits the training data too closely, but does not generalize well to new data. L1 regularization is like putting your toys in the box in a way that only allows a certain number of toys. You might choose to keep your favorite toys and leave out the ones you don’t play with as much.\nL2 regularization tries to simplify a model by shrinking its coefficients towards zero, without setting them to exactly zero. This helps prevent overfitting and also improves the stability of the model. L2 regularization is like putting your toys in the box in a way that allows all of them to fit, but not too tightly. You might arrange the toys so that they are evenly spaced and not too close together.\nIt does this by adding a cost function (penalized weight)!\n\n\n\nBuilding an xgboost model\nJust like for all the previous modelling approaches, xgboost requires parameterisation. These are:\n\ntrees: The number of decision trees to create in the ensemble. A higher value will typically result in better performance, but can also increase the risk of overfitting. A typical starting value could be around 100, and then increase or decrease based on the model’s performance.\ntree_depth: The maximum depth of each decision tree. A higher value will allow the model to capture more complex relationships in the data, but can also increase the risk of overfitting. A typical starting value could be around 6 or 8, and then increase or decrease based on the model’s performance.\nmin_n: The minimum number of observations that must be present in a leaf node of a decision tree. A higher value can prevent overfitting, but may also result in underfitting. A typical starting value could be around 10 or 20, and then increase or decrease based on the model’s performance.\nloss_reduction: The minimum amount of loss reduction required to split a node in a decision tree. A higher value can prevent overfitting, but may also result in underfitting. A typical starting value could be around 0.1, and then increase or decrease based on the model’s performance.\nsample_size: The fraction of observations to randomly sample for each tree. A lower value can prevent overfitting, but may also result in underfitting. A typical starting value could be around 0.7 or 0.8, and then increase or decrease based on the model’s performance.\nmtry: The number of variables to randomly select for each split in a decision tree. A higher value can result in better performance, but can also increase the risk of overfitting. A typical starting value could be around the square root of the number of features, and then increase or decrease based on the model’s performance.\nlearn_rate: The step size to use when updating the weights in each iteration. A lower value can result in better performance, but may also require more iterations to converge. A typical starting value could be around 0.1, and then increase or decrease based on the model’s performance.\n\nP/S: Heuristics are just guidelines and may not be the best solution for your data/question.\n\n\nTutorial 3: Fitting an xgboost model\nYou have 5 minutes. Take into consideration: accuracy, computation, and intepretability.\nThere is a heuristic model, and three other models. Lets see who gets the best model in the alotted time.\n\n#define a recipe\nxgboost_recipe <- recipe(undx_dm~., data=train_tbl) %>%\n  step_novel(all_predictors(), -all_numeric()) %>% \n  step_string2factor(all_nominal(), -all_outcomes()) %>%\n  step_dummy(income, one_hot = T) %>%\n  step_dummy(smoke, one_hot = T) %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  step_impute_mean(smoke_Never, smoke_Former, smoke_Current) %>%\n  step_mutate(sex=as.numeric(sex),\n         residential=as.numeric(sex)) %>%\n  step_zv(all_predictors())\n\n#Heuristic model\n#specify the model\nxgboost_spec <- \n  boost_tree(\n  trees = 100,\n  tree_depth = 6,\n  min_n = 10,\n  loss_reduction = 0.01,\n  sample_size = 0.7,\n  mtry = 3,\n  learn_rate = 0.1) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n#define the workflow\nxgboost_workflow <- \n  workflow() %>% \n  add_recipe(xgboost_recipe) %>% \n  add_model(xgboost_spec)\n#run the model\nxgb_heu <- xgboost_workflow %>% \n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#Model 1\n#specify the model\nxgboost_spec <- \n  boost_tree(\n  trees = 200,\n  tree_depth = 6,\n  min_n = 10,\n  loss_reduction = 0.01,\n  sample_size = 0.8,\n  mtry = 3,\n  learn_rate = 0.25) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n#define the workflow\nxgboost_workflow <- \n  workflow() %>% \n  add_recipe(xgboost_recipe) %>% \n  add_model(xgboost_spec)\n#run the model\nxgb_m1 <- xgboost_workflow %>% \n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#Model 2\n#specify the model\nxgboost_spec <- \n  boost_tree(\n  trees = 500,\n  tree_depth = 6,\n  min_n = 10,\n  loss_reduction = 0.2,\n  sample_size = 0.9,\n  mtry = 3,\n  learn_rate = 0.5) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n#define the workflow\nxgboost_workflow <- \n  workflow() %>% \n  add_recipe(xgboost_recipe) %>% \n  add_model(xgboost_spec)\n#run the model\nxgb_m2 <- xgboost_workflow %>% \n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#Model 3\n#specify the model\nxgboost_spec <- \n  boost_tree(\n  trees = 1000,\n  tree_depth = 8,\n  min_n = 10,\n  loss_reduction = 0.7,\n  sample_size = 1,\n  mtry = 3,\n  learn_rate = 1) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n#define the workflow\nxgboost_workflow <- \n  workflow() %>% \n  add_recipe(xgboost_recipe) %>% \n  add_model(xgboost_spec)\n#run the model\nxgb_m3 <- xgboost_workflow %>% \n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#predict values\npredictions_xgb_heu <- xgb_heu %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm)) %>%\n  mutate(undx_dm=as.factor(undx_dm))\npredictions_xgb_m1 <- xgb_m1 %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm)) %>%\n  mutate(undx_dm=as.factor(undx_dm))\npredictions_xgb_m2 <- xgb_m2 %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm)) %>%\n  mutate(undx_dm=as.factor(undx_dm))\npredictions_xgb_m3 <- xgb_m3 %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm)) %>%\n  mutate(undx_dm=as.factor(undx_dm))\n\n#define metrics\neval_metrics <- metric_set(accuracy, ppv, recall, specificity, f_meas)\n#call metrics on models\neval_metrics_xgb_heu <- eval_metrics(data = predictions_xgb_heu, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Xgboost Heuristic Model\"=\".estimate\")\neval_metrics_xgb_m1 <- eval_metrics(data = predictions_xgb_m1, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Xgboost Model 1\"=\".estimate\")\neval_metrics_xgb_m2 <- eval_metrics(data = predictions_xgb_m2, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Xgboost Model 2\"=\".estimate\")\neval_metrics_xgb_m3 <- eval_metrics(data = predictions_xgb_m3, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Xgboost Model 3\"=\".estimate\")\n\n#put them together\nleft_join(eval_metrics_xgb_heu, eval_metrics_xgb_m1, by=\".metric\") %>%\n  left_join(eval_metrics_xgb_m2, by=\".metric\") %>%\n  left_join(eval_metrics_xgb_m3, by=\".metric\") %>%\n  kable()\n\n\n\n \n  \n    .metric \n    Xgboost Heuristic Model \n    Xgboost Model 1 \n    Xgboost Model 2 \n    Xgboost Model 3 \n  \n \n\n  \n    accuracy \n    0.9010000 \n    0.8910000 \n    0.8700000 \n    0.8680000 \n  \n  \n    ppv \n    0.9010000 \n    0.9008097 \n    0.9011446 \n    0.9034627 \n  \n  \n    recall \n    1.0000000 \n    0.9877913 \n    0.9611543 \n    0.9556049 \n  \n  \n    specificity \n    0.0000000 \n    0.0101010 \n    0.0404040 \n    0.0707071 \n  \n  \n    f_meas \n    0.9479221 \n    0.9422975 \n    0.9301826 \n    0.9288026"
  },
  {
    "objectID": "index.html#cross-validation",
    "href": "index.html#cross-validation",
    "title": "An Introduction to Machine Learning in R",
    "section": "Cross Validation",
    "text": "Cross Validation\nCross-validation is a statistical method used to evaluate the performance of a machine learning model on a limited dataset. The common types of cross validation are k-fold cross validation, leave-one-out cross validation, stratified cross validation, and nested cross validation.\nFor posterity we will limit ourselves to k-fold here. K-fold cross validation helps us avoid overfitting (when a model performs well on the training data but poorly on new data) and gives us a more reliable estimate of how well our model will generalize to new data. These are steps used in k-folds:\n\nWe divide our dataset into k subsets or “folds” of approximately equal size.\nWe train our model on k-1 folds of the data, and then use the remaining fold as the validation set to evaluate the model’s performance.\nWe repeat this process k times, with each fold serving as the validation set exactly once.\nFinally, we average the performance across all k folds to get an overall estimate of how well our model is likely to perform on new, unseen data.\n\n\n\n\nK-folds cross validation (Source: Jian et al, 2022)\n\n\nGrid search is a technique used to find the best combination of hyperparameters (settings for a machine learning model that are not learned from the data, but rather set by the user) for a given model.\nHere’s how it works:\n\nWe define a set of hyperparameters to explore and their possible values.\nWe then create a grid of all possible combinations of these hyperparameters.\nFor each combination of hyperparameters, we perform k-fold cross validation to evaluate the performance of the model.\nWe choose the combination of hyperparameters that gives the best performance.\n\nHyperparameter tuning is the process of finding the best combination of hyperparameters for a given machine learning model.\nUsing k-fold cross validation during grid search and hyperparameter tuning ensures that we are not overfitting our model to a particular set of hyperparameters, and gives us a more accurate estimate of how well our model is likely to perform on new, unseen data.\n\n\n\n\nRandom Forest CV\n\nSet up the model\nSo lets recap on the model we have built so far. One trick we havent learnt so far is to shorten the amount of code needed for training and testing. We can use the last_fit() function in from tidymodels to train on the train data and automatically test it using the original train-test split.\n\n#define a recipe\nrf_recipe <- recipe(undx_dm~., data=train_tbl) %>%\n  step_novel(all_predictors(), -all_numeric()) %>% \n  step_string2factor(all_nominal(), -all_outcomes()) %>%\n  step_dummy(income, one_hot = T) %>%\n  step_dummy(smoke, one_hot = T) %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  step_impute_mean(smoke_Never, smoke_Former, smoke_Current) %>%\n  step_zv(all_predictors())\n\n#specify the model\nrf_spec <- \n  rand_forest(mtry = 4,#specifies the number of variables that are randomly sampled as candidates at each split in the tree\n              trees = 500,#specifies the number of trees in the forest\n              min_n = 5,#minimum number of observations required to split a node\n              mode=\"classification\") %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n#define a workflow\nrf_workflow<- workflow() %>% \n  add_recipe(rf_recipe) %>% \n  add_model(rf_spec)\n\n#run the model\nrandom_forest_base <- rf_workflow %>% \n  last_fit(train_test_split)\n\n! train/test split: preprocessor 1/1: There are new levels in a factor: NA\n\n#check accuracy \n#check the rocjand accuracy\nrf_metrics_base <- random_forest_base %>% \n  collect_metrics() %>% select(.metric, .estimate) %>%\n  rename(base=.estimate)\n\nLet now how we can tune this model using CV. First lets specify the hyperparameters we would like to tune.\n\n#specify the model\nrf_spec <- \n  rand_forest(mtry = tune(),\n              trees = 1000,\n              min_n = tune(),\n              mode=\"classification\") %>%\n  set_engine(\"ranger\") %>%\n  set_mode(\"classification\")\n\n#define a workflow\nrf_workflow<- workflow() %>% \n  add_recipe(rf_recipe) %>% \n  add_model(rf_spec)\n\n\n\nTrain hyperparameters\nSet up the CV resamples for tuning\n\nset.seed(234)\nrf_folds <- vfold_cv(train_tbl)\n\nSet up parallel processing to speed up the process. Multiple grid points can be process independently and simultaneously.\n\ndoParallel::registerDoParallel()\n\nset.seed(345)\nrf_tune <- tune_grid(\n  rf_workflow,\n  resamples = rf_folds,\n  grid = 20\n)\n\ni Creating pre-processing data to finalize unknown parameter: mtry\n\n\nWarning: There are new levels in a factor: NA\n\nrf_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics          .notes           \n   <list>             <chr>  <list>            <list>           \n 1 <split [3600/400]> Fold01 <tibble [40 × 6]> <tibble [1 × 3]> \n 2 <split [3600/400]> Fold02 <tibble [40 × 6]> <tibble [1 × 3]> \n 3 <split [3600/400]> Fold03 <tibble [40 × 6]> <tibble [1 × 3]> \n 4 <split [3600/400]> Fold04 <tibble [40 × 6]> <tibble [1 × 3]> \n 5 <split [3600/400]> Fold05 <tibble [40 × 6]> <tibble [1 × 3]> \n 6 <split [3600/400]> Fold06 <tibble [40 × 6]> <tibble [20 × 3]>\n 7 <split [3600/400]> Fold07 <tibble [40 × 6]> <tibble [1 × 3]> \n 8 <split [3600/400]> Fold08 <tibble [40 × 6]> <tibble [1 × 3]> \n 9 <split [3600/400]> Fold09 <tibble [40 × 6]> <tibble [1 × 3]> \n10 <split [3600/400]> Fold10 <tibble [40 × 6]> <tibble [1 × 3]> \n\nThere were issues with some computations:\n\n  - Warning(s) x29: There are new levels in a factor: NA\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nWe can visualise this validation set using AUC\n\nrf_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  select(mean, min_n, mtry) %>%\n  pivot_longer(min_n:mtry,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n\n\n\n\nThe grid search is not exhaustive in that it does not check every combination of hyperparameters. It however gives us a good idea of what values could work better. Wider ranges require greater computation. In this example and considering the AUCs are rather narrow- lets focus on a short range.\n\nrf_grid <- grid_regular(\n  mtry(range = c(3, 7)),\n  min_n(range = c(30, 35)),\n  levels = 5\n)\n\nrf_grid\n\n# A tibble: 25 × 2\n    mtry min_n\n   <int> <int>\n 1     3    30\n 2     4    30\n 3     5    30\n 4     6    30\n 5     7    30\n 6     3    31\n 7     4    31\n 8     5    31\n 9     6    31\n10     7    31\n# … with 15 more rows\n\n\nNow we can tune again but using a more refined search grid.\n\nset.seed(456)\nrf_tune <- tune_grid(\n  rf_workflow,\n  resamples = rf_folds,\n  grid = rf_grid\n)\n\nrf_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics          .notes           \n   <list>             <chr>  <list>            <list>           \n 1 <split [3600/400]> Fold01 <tibble [50 × 6]> <tibble [1 × 3]> \n 2 <split [3600/400]> Fold02 <tibble [50 × 6]> <tibble [1 × 3]> \n 3 <split [3600/400]> Fold03 <tibble [50 × 6]> <tibble [1 × 3]> \n 4 <split [3600/400]> Fold04 <tibble [50 × 6]> <tibble [1 × 3]> \n 5 <split [3600/400]> Fold05 <tibble [50 × 6]> <tibble [1 × 3]> \n 6 <split [3600/400]> Fold06 <tibble [50 × 6]> <tibble [25 × 3]>\n 7 <split [3600/400]> Fold07 <tibble [50 × 6]> <tibble [1 × 3]> \n 8 <split [3600/400]> Fold08 <tibble [50 × 6]> <tibble [1 × 3]> \n 9 <split [3600/400]> Fold09 <tibble [50 × 6]> <tibble [1 × 3]> \n10 <split [3600/400]> Fold10 <tibble [50 × 6]> <tibble [1 × 3]> \n\nThere were issues with some computations:\n\n  - Warning(s) x34: There are new levels in a factor: NA\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nLets see how we did this time\n\nrf_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  mutate(min_n = factor(min_n)) %>%\n  ggplot(aes(mtry, mean, color = min_n)) +\n  geom_line(alpha = 0.5, size = 1.5) +\n  geom_point() +\n  labs(y = \"AUC\")\n\n\n\n\nNow that actually is really interesting and gives a mtry of 5 and min_n of 33. However imagine if we had way more parameters than this. We can just ask the model to find the best parameters and bake it into our model specification.\n\nrf_auc <- select_best(rf_tune, \"roc_auc\")\n\nrf_cv <- finalize_model(\n  rf_spec,\n  rf_auc\n)\n\nrf_cv\n\nRandom Forest Model Specification (classification)\n\nMain Arguments:\n  mtry = 7\n  trees = 1000\n  min_n = 32\n\nComputational engine: ranger \n\n\nExcellent! Now lets train the final model and compare to the base model.\n\n#redefine a workflow\nrf_workflow<- workflow() %>% \n  add_recipe(rf_recipe) %>% \n  add_model(rf_cv)\n\n#run the model\nrandom_forest_final <- rf_workflow %>% \n  last_fit(train_test_split)\n\n! train/test split: preprocessor 1/1: There are new levels in a factor: NA\n\n#check the rocjand accuracy\nrf_metrics_final <- random_forest_final %>%\n  collect_metrics() %>% select(.metric, .estimate) %>%\n  rename(final=.estimate)\n\n#compare the models\nrf_metrics <- left_join(rf_metrics_base, rf_metrics_final, by=\".metric\")\nrf_metrics\n\n# A tibble: 2 × 3\n  .metric   base final\n  <chr>    <dbl> <dbl>\n1 accuracy 0.901 0.901\n2 roc_auc  0.449 0.440\n\n\n\n\n\nSupport Vector Machines CV\n\nSet up the model\nSimilar to the random forest story, lets set up a base model for SVM\n\n#define a recipe\nsvm_recipe <- recipe(undx_dm~., data=train_tbl) %>%\n  step_novel(all_predictors(), -all_numeric()) %>% \n  step_string2factor(all_nominal(), -all_outcomes()) %>%\n  step_dummy(income, one_hot = T) %>%\n  step_dummy(smoke, one_hot = T) %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  step_impute_mean(smoke_Never, smoke_Former, smoke_Current) %>%\n  step_mutate(sex=as.numeric(sex),\n         residential=as.numeric(sex)) %>%\n  step_zv(all_predictors())\n\n# Define the tuning specification\nsvm_spec <- svm_poly(\n  cost = 0.25, \n  degree = 1) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n\n# Workflow for tuning\nsvm_workflow <- \n  workflow() %>%\n  add_recipe(svm_recipe) %>%\n  # add the tuning specificiatons\n  add_model(svm_spec)\n\n#run the model\nsvm_base <-  svm_workflow %>% \n  last_fit(train_test_split)\n\n! train/test split: preprocessor 1/1: There are new levels in a factor: NA\n\n#check accuracy \n#check the rocjand accuracy\nsvm_metrics_base <- svm_base %>% \n  collect_metrics() %>% select(.metric, .estimate) %>%\n  rename(base=.estimate)\n\nLet now how we can tune this model using CV. First lets specify the hyperparameters we would like to tune.\n\n# Define the tuning specification\nsvm_spec <- svm_poly(\n  cost = tune(), \n  degree = tune()) %>%\n  set_mode(\"classification\") %>%\n  set_engine(\"kernlab\")\n\n# Workflow for tuning\nsvm_workflow <- \n  workflow() %>%\n  add_recipe(svm_recipe) %>%\n  add_model(svm_spec)\n\n\n\nTrain hyperparameters\nSet up the CV resamples for tuning\n\nset.seed(234)\nsvm_folds <- vfold_cv(train_tbl)\n\nSet up parallel processing to speed up the process. Multiple grid points can be process independently and simultaneously.\n\ndoParallel::registerDoParallel()\n\nset.seed(345)\nsvm_tune <- tune_grid(\n  svm_workflow,\n  resamples = svm_folds,\n  grid = 20\n)\n\nsvm_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics          .notes           \n   <list>             <chr>  <list>            <list>           \n 1 <split [3600/400]> Fold01 <tibble [40 × 6]> <tibble [1 × 3]> \n 2 <split [3600/400]> Fold02 <tibble [40 × 6]> <tibble [1 × 3]> \n 3 <split [3600/400]> Fold03 <tibble [38 × 6]> <tibble [2 × 3]> \n 4 <split [3600/400]> Fold04 <tibble [38 × 6]> <tibble [2 × 3]> \n 5 <split [3600/400]> Fold05 <tibble [36 × 6]> <tibble [3 × 3]> \n 6 <split [3600/400]> Fold06 <tibble [40 × 6]> <tibble [20 × 3]>\n 7 <split [3600/400]> Fold07 <tibble [38 × 6]> <tibble [2 × 3]> \n 8 <split [3600/400]> Fold08 <tibble [38 × 6]> <tibble [2 × 3]> \n 9 <split [3600/400]> Fold09 <tibble [40 × 6]> <tibble [1 × 3]> \n10 <split [3600/400]> Fold10 <tibble [38 × 6]> <tibble [2 × 3]> \n\nThere were issues with some computations:\n\n  - Error(s) x7: Error in prob.model(object)[[p]]$A: $ operator is invalid for ato...   - Warning(s) x29: Error in prob.model(object)[[p]]$A: $ operator is invalid for ato...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nWe can visualise this validation set using AUC\n\nsvm_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  select(mean, cost, degree) %>%\n  pivot_longer(cost:degree,\n    values_to = \"value\",\n    names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n\n\n\n\nThe grid search is not exhaustive in that it does not check every combination of hyperparameters. It however gives us a good idea of what values could work better. Wider ranges require greater computation. In this example and considering the AUCs are rather narrow- lets focus on a short range.\n\nsvm_grid <- grid_regular(\n  cost(range = c(0, 12)),\n  degree(range = c(0,1)),\n  levels = 5\n)\n\nsvm_grid\n\n# A tibble: 25 × 2\n    cost degree\n   <dbl>  <dbl>\n 1     1   0   \n 2     8   0   \n 3    64   0   \n 4   512   0   \n 5  4096   0   \n 6     1   0.25\n 7     8   0.25\n 8    64   0.25\n 9   512   0.25\n10  4096   0.25\n# … with 15 more rows\n\n\nNow we can tune again but using a more refined search grid.\n\nset.seed(456)\nsvm_tune <- tune_grid(\n  svm_workflow,\n  resamples = svm_folds,\n  grid = svm_grid\n)\n\nsvm_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 4\n   splits             id     .metrics          .notes           \n   <list>             <chr>  <list>            <list>           \n 1 <split [3600/400]> Fold01 <tibble [20 × 6]> <tibble [16 × 3]>\n 2 <split [3600/400]> Fold02 <tibble [20 × 6]> <tibble [16 × 3]>\n 3 <split [3600/400]> Fold03 <tibble [20 × 6]> <tibble [16 × 3]>\n 4 <split [3600/400]> Fold04 <tibble [20 × 6]> <tibble [16 × 3]>\n 5 <split [3600/400]> Fold05 <tibble [20 × 6]> <tibble [16 × 3]>\n 6 <split [3600/400]> Fold06 <tibble [20 × 6]> <tibble [25 × 3]>\n 7 <split [3600/400]> Fold07 <tibble [20 × 6]> <tibble [16 × 3]>\n 8 <split [3600/400]> Fold08 <tibble [20 × 6]> <tibble [16 × 3]>\n 9 <split [3600/400]> Fold09 <tibble [20 × 6]> <tibble [16 × 3]>\n10 <split [3600/400]> Fold10 <tibble [20 × 6]> <tibble [16 × 3]>\n\nThere were issues with some computations:\n\n  - Error(s) x150: Error in votematrix[i, ret < 0] <- votematrix[i, ret < 0] + 1: NA...   - Warning(s) x19: Error in votematrix[i, ret < 0] <- votematrix[i, ret < 0] + 1: NA...\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nLets see how we did this time\n\nsvm_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  mutate(degree = factor(degree)) %>%\n  ggplot(aes(cost, mean, color = degree)) +\n  geom_line(alpha = 0.5, size = 1.5) +\n  geom_point() +\n  labs(y = \"AUC\")\n\n\n\n\nNow for SVM we certainly see that the parameters are more difficult to intepret. Lets leave to the the algorithm.\n\nsvm_auc <- select_best(svm_tune, \"roc_auc\")\n\nsvm_cv <- finalize_model(\n  svm_spec,\n  svm_auc\n)\n\nsvm_cv\n\nPolynomial Support Vector Machine Model Specification (classification)\n\nMain Arguments:\n  cost = 4096\n  degree = 1\n\nComputational engine: kernlab \n\n\nExcellent! Now lets train the final model and compare to the base model.\n\n#redefine a workflow\nsvm_workflow<- workflow() %>% \n  add_recipe(svm_recipe) %>% \n  add_model(svm_cv)\n\n#run the model\nrandom_forest_final <- svm_workflow %>% \n  last_fit(train_test_split)\n\n! train/test split: preprocessor 1/1: There are new levels in a factor: NA\n\n#check the rocjand accuracy\nsvm_metrics_final <- random_forest_final %>%\n  collect_metrics() %>% select(.metric, .estimate) %>%\n  rename(final=.estimate)\n\n#compare the models\nsvm_metrics <- left_join(svm_metrics_base, svm_metrics_final, by=\".metric\")\nsvm_metrics\n\n# A tibble: 2 × 3\n  .metric   base final\n  <chr>    <dbl> <dbl>\n1 accuracy 0.901 0.901\n2 roc_auc  0.499 0.510\n\n\n\n\n\nXGBoost CV\n\nSet up the model\nAnd finally lets set up a base model for xgboost\n\n#define a recipe\nxgb_recipe <- recipe(undx_dm~., data=train_tbl) %>%\n  step_novel(all_predictors(), -all_numeric()) %>% \n  step_string2factor(all_nominal(), -all_outcomes()) %>%\n  step_dummy(income, one_hot = T) %>%\n  step_dummy(smoke, one_hot = T) %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  step_impute_mean(smoke_Never, smoke_Former, smoke_Current) %>%\n  step_mutate(sex=as.numeric(sex),\n         residential=as.numeric(sex)) %>%\n  step_zv(all_predictors())\n\n#specify the model\nxgb_spec <- \n  boost_tree(\n  trees = 100,\n  tree_depth = 6,\n  min_n = 10,\n  loss_reduction = 0.01,\n  sample_size = 0.7,\n  mtry = 3,\n  learn_rate = 0.1) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n\n#define the workflow\nxgb_workflow <- \n  workflow() %>% \n  add_recipe(xgb_recipe) %>% \n  add_model(xgb_spec)\n\n#run the model\nxgb_base <- xgb_workflow %>% \n  last_fit(train_test_split)\n\n! train/test split: preprocessor 1/1: There are new levels in a factor: NA\n\n#check accuracy \n#check the rocjand accuracy\nxgb_metrics_base <- xgb_base %>% \n  collect_metrics() %>% select(.metric, .estimate) %>%\n  rename(base=.estimate)\n\nLet now how we can tune this model using CV. First lets specify the hyperparameters we would like to tune.\n\n##specify the model\nxgb_spec <- \n  boost_tree(\n  trees = 1000,\n  tree_depth = tune(),\n  min_n = tune(),\n  loss_reduction = tune(),\n  sample_size = tune(),\n  mtry = tune(),\n  learn_rate = tune()) %>% \n  set_mode(\"classification\") %>% \n  set_engine(\"xgboost\") \n\n#define the workflow\nxgb_workflow <- \n  workflow() %>% \n  add_recipe(xgb_recipe) %>% \n  add_model(xgb_spec)\n\n\n\nTrain hyperparameters\nSet up the CV resamples for tuning\n\nset.seed(234)\nxgb_folds <- vfold_cv(train_tbl)\n\nThe tuning process in xgboost is technically similar to random forest- however it is not. This is due to the sheer number of parameters that require tuning with xgboost. As such a space filling design is utilised instead of the usual grid selection method. This is implemented using the grid_latin_hypercube function.\n\nxgb_grid <- grid_latin_hypercube(\n  tree_depth(),\n  min_n(),\n  loss_reduction(),\n  sample_size = sample_prop(),\n  finalize(mtry(), train_tbl),\n  learn_rate(),\n  size = 30\n)\n\nxgb_grid\n\n# A tibble: 30 × 6\n   tree_depth min_n loss_reduction sample_size  mtry learn_rate\n        <int> <int>          <dbl>       <dbl> <int>      <dbl>\n 1         13    25       3.07e- 8       0.948     3   3.89e- 4\n 2         14    29       1.37e-10       0.283    13   8.42e- 9\n 3         14     7       5.42e- 5       0.413     4   1.50e- 6\n 4          6    25       4.80e+ 0       0.640    11   1.76e-10\n 5          2    38       6.85e- 5       0.764     9   4.85e-10\n 6         12    21       1.20e- 3       0.176    11   9.68e- 6\n 7          7    35       1.62e- 8       0.201    10   3.58e- 5\n 8          9    33       1.93e- 1       0.242    10   1.98e- 6\n 9         10    22       6.45e- 7       0.711     2   3.69e- 6\n10         11    32       2.73e- 4       0.453     3   1.63e- 8\n# … with 20 more rows\n\n\nAs we did previously lets tune parallel to speed up the process\n\ndoParallel::registerDoParallel()\n\nset.seed(234)\nxgb_tune <- tune_grid(\n  xgb_workflow,\n  resamples = xgb_folds,\n  grid = xgb_grid,\n  control = control_grid(save_pred = TRUE)\n)\n\nxgb_tune\n\n# Tuning results\n# 10-fold cross-validation \n# A tibble: 10 × 5\n   splits             id     .metrics           .notes            .predictions\n   <list>             <chr>  <list>             <list>            <list>      \n 1 <split [3600/400]> Fold01 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n 2 <split [3600/400]> Fold02 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n 3 <split [3600/400]> Fold03 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n 4 <split [3600/400]> Fold04 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n 5 <split [3600/400]> Fold05 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n 6 <split [3600/400]> Fold06 <tibble [60 × 10]> <tibble [30 × 3]> <tibble>    \n 7 <split [3600/400]> Fold07 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n 8 <split [3600/400]> Fold08 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n 9 <split [3600/400]> Fold09 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n10 <split [3600/400]> Fold10 <tibble [60 × 10]> <tibble [1 × 3]>  <tibble>    \n\nThere were issues with some computations:\n\n  - Warning(s) x39: There are new levels in a factor: NA\n\nRun `show_notes(.Last.tune.result)` for more information.\n\n\nWe can visualise the hyperparameter grid search to better understand our results as well\n\nxgb_tune %>%\n  collect_metrics() %>%\n  filter(.metric == \"roc_auc\") %>%\n  select(mean, mtry:sample_size) %>%\n  pivot_longer(mtry:sample_size,\n               values_to = \"value\",\n               names_to = \"parameter\"\n  ) %>%\n  ggplot(aes(value, mean, color = parameter)) +\n  geom_point(alpha = 0.8, show.legend = FALSE) +\n  facet_wrap(~parameter, scales = \"free_x\") +\n  labs(x = NULL, y = \"AUC\")\n\n\n\n\nWe can visualise the diverse ways really our model can be tuned but lets leave the selection to the function select_best instead of going through the trouble of manually figuring out which is best.\n\nxgb_auc <- select_best(xgb_tune, \"roc_auc\")\n\nxgb_cv <- finalize_model(\n  xgb_spec,\n  xgb_auc\n)\n\nxgb_cv\n\nBoosted Tree Model Specification (classification)\n\nMain Arguments:\n  mtry = 10\n  trees = 1000\n  min_n = 5\n  tree_depth = 13\n  learn_rate = 0.00170249958923571\n  loss_reduction = 0.0135502138897195\n  sample_size = 0.755651205317117\n\nComputational engine: xgboost \n\n\nExcellent! Now lets train the final model and compare to the base model.\n\n#redefine a workflow\nxgb_workflow<- workflow() %>% \n  add_recipe(xgb_recipe) %>% \n  add_model(xgb_cv)\n\n#run the model\nxgb_final <- xgb_workflow %>% \n  last_fit(train_test_split)\n\n! train/test split: preprocessor 1/1: There are new levels in a factor: NA\n\n#check the rocjand accuracy\nxgb_metrics_final <- xgb_final %>%\n  collect_metrics() %>% select(.metric, .estimate) %>%\n  rename(final=.estimate)\n\n#compare the models\nxgb_metrics <- left_join(xgb_metrics_base, xgb_metrics_final, by=\".metric\")\nxgb_metrics\n\n# A tibble: 2 × 3\n  .metric   base final\n  <chr>    <dbl> <dbl>\n1 accuracy 0.901 0.901\n2 roc_auc  0.497 0.454"
  },
  {
    "objectID": "index.html#model-evaluation",
    "href": "index.html#model-evaluation",
    "title": "An Introduction to Machine Learning in R",
    "section": "Model Evaluation",
    "text": "Model Evaluation\nIn the above examples we ‘shortcutted’ a step by using the last_fit function. Let us rebuild the models to evaluate our models stepwise.\n\n#run the model\nrf_final <- rf_workflow %>% \n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#run the model\nsvm_final <- svm_workflow %>% \n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#run the model\nxgb_final <- xgb_workflow %>% \n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n\nEvaluation is where the yardstick packages comes in. yardstick provide a simple way to calculate several popular assessment measures. But before we can do that we’ll need some predictions. We get our predictions by passing the test_baked data to the predict function\n\npredictions_glm <- logistic_glm %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm)) %>%\n  mutate(undx_dm=as.factor(undx_dm))\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\npredictions_rf <- rf_final %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\n\npredictions_svm <- svm_final %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\n\npredictions_xgb <- xgb_final %>%\n      predict(new_data = test_tbl) %>%\n      bind_cols(test_tbl %>% select(undx_dm))%>%\n  mutate(undx_dm=as.factor(undx_dm))\n\nNow with our freshly minted predictions there are several metrics that can be used to evaluate the performance of our classification model.\n\nConfusion matrix\nThe confusion matrix classifies cases between two binary categories, category 1 for patients who tested positive for diabetes and category 0 for patients who tested negative.\n\nplot_glm <- \n  predictions_glm %>%\n  conf_mat(undx_dm, .pred_class) %>%\n  pluck(1) %>%\n  as_tibble() %>%\n  ggplot(aes(Prediction, Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8) +\n  ggtitle(\"Logistic Regression\")\n\nplot_rf <- \n  predictions_rf %>%\n  conf_mat(undx_dm, .pred_class) %>%\n  pluck(1) %>%\n  as_tibble() %>%\n  ggplot(aes(Prediction, Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)+\n  ggtitle(\"Random Forest\")\n\nplot_svm <- \n  predictions_svm %>%\n  conf_mat(undx_dm, .pred_class) %>%\n  pluck(1) %>%\n  as_tibble() %>%\n  ggplot(aes(Prediction, Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)+\n  ggtitle(\"Support Vector Machines\")\n\nplot_xgb <- \n  predictions_xgb %>%\n  conf_mat(undx_dm, .pred_class) %>%\n  pluck(1) %>%\n  as_tibble() %>%\n  ggplot(aes(Prediction, Truth, alpha = n)) +\n  geom_tile(show.legend = FALSE) +\n  geom_text(aes(label = n), colour = \"white\", alpha = 1, size = 8)+\n  ggtitle(\"xgBoost\")\n\nplot_grid(plot_glm, plot_rf, plot_svm, plot_xgb, nrow = 2)\n\n\n\n\nUsing the GLM model as an example If your model predicts a patient as 1 (positive) and they belong to category 1 (positive) in reality we call this a true positive, shown by the top left number 112.\nIf your model predicts a patient as 0 (negative) and they belong to category 1 (positive) in reality we call this a false negative, shown by the bottom left number 191\nIf your model predicts a patient as 1 (positive) and they belong to category 0 (negative) in reality we call this a false positive, shown by the top right number 73.\nIf your model predicts a patient as 0 (negative) and they belong to category 0 (negative) in reality we call this a true negative, shown by the bottom right number 2429\nOur confusion matrix can thus be expressed in the following form:\n| | | Predicted | Predicted | |\n|-------|-----|-----------|-----------|---|\n| | | Yes | No | |\n| Truth | Yes | 112 (TP) | 191 (FN) | |\n| Truth | No | 73 (FP) | 2429 (TN) | |\nAs you might have guessed it’s preferable to have a larger number of true positives and true negatives and a lower number of false positives and false negatives, which implies that the model performs better. Now with just a few more lines of code with have a Confusion Matrix that can be leveraged to calculate additional metrics. We will be looking at several important measures such as:\n\nAccuracy: TP + TN/(TP + TN + FP + FN) The percentage of labels predicted accurately for a sample. The model’s Accuracy is the fraction of predictions the model got right and can be easily calculated by passing the predictions_glm to the metrics function. However, accuracy is not a very reliable metric as it will provide misleading results if the data set is unbalanced.\nPrecision and Recall:\n\nPrecision (PPV): TP/(TP + FP) defined as the proportion of predicted positives that are actually positive. Also called positive predictive value\nRecall (Sensitivity): TP/(TP + FN) defined as the proportion of positive results out of the number of samples which were actually positive.\n\n\nPrecision shows how sensitive models are to False Positives (i.e. predicting a customer is leaving when he-she is actually staying) whereas Recall looks at how sensitive models are to False Negatives.\nThese are very relevant business metrics because organisations are particularly interested in accurately predicting which customers are truly at risk of leaving so that they can target them with retention strategies. At the same time they want to minimising efforts of retaining customers incorrectly classified as leaving who are instead staying.\n\nSpecificity: TN/(TN + FP) defined as the proportion of negative results out of the number of samples which were actually negative.\nF1 Score: A weighted average of the precision and recall, with best being 1 and worst being 0. The F1 Score is the harmonic average of the precision and recall. An F1 score reaches its best value at 1 with perfect precision and recall.\n\nTidymodels provides yet another succinct way of evaluating all these metrics. Using yardstick::metric_set(), you can combine multiple metrics together into a new function that calculates all of them at once.\n\n#define metrics\neval_metrics <- metric_set(accuracy, ppv, recall, specificity, f_meas)\n\n#call metrics on models\neval_metrics_glm <- eval_metrics(data = predictions_glm, truth = undx_dm, estimate = .pred_class) %>%\n  select(-.estimator) %>%\n  rename(\"Logistic regression\"=\".estimate\")\n\neval_metrics_rf <- eval_metrics(data = predictions_rf, truth = undx_dm, estimate = .pred_class)%>%\n  select(-.estimator) %>%\n  rename(\"Random forest\"=\".estimate\")\n\neval_metrics_xgb <- eval_metrics(data = predictions_xgb, truth = undx_dm, estimate = .pred_class)%>%\n  select(-.estimator) %>%\n  rename(\"xgBoost\"=\".estimate\")\n\neval_metrics_svm <- eval_metrics(data = predictions_svm, truth = undx_dm, estimate = .pred_class)%>%\n  select(-.estimator) %>%\n  rename(\"Supervised vector machines\"=\".estimate\")\n\n#put them together\nleft_join(eval_metrics_glm, eval_metrics_rf, by=\".metric\") %>%\n  left_join(eval_metrics_xgb, by=\".metric\") %>%\n  left_join(eval_metrics_svm, by=\".metric\") %>%\n  kable()\n\n\n\n \n  \n    .metric \n    Logistic regression \n    Random forest \n    xgBoost \n    Supervised vector machines \n  \n \n\n  \n    accuracy \n    0.9010000 \n    0.9010000 \n    0.9010000 \n    0.9010000 \n  \n  \n    ppv \n    0.9010000 \n    0.9010000 \n    0.9010000 \n    0.9010000 \n  \n  \n    recall \n    1.0000000 \n    1.0000000 \n    1.0000000 \n    1.0000000 \n  \n  \n    specificity \n    0.0000000 \n    0.0000000 \n    0.0000000 \n    0.0000000 \n  \n  \n    f_meas \n    0.9479221 \n    0.9479221 \n    0.9479221 \n    0.9479221 \n  \n\n\n\n\n\nUsing the precision (ppv) metric, we are able to answer the question: - Of all the patients the model predicted are diabetic, how many are actually diabetic?\nUsing the recall metric, we are able to answer the question: - Of all the patients that are actually diabetic, how many did the model identify?\n\n\nROC Curve\nUntil now, we’ve considered the predictions from the model as being either 1 or 0 class labels. Actually, things are a little more complex than that. Statistical machine learning algorithms, like logistic regression, are based on probability; so what actually gets predicted by a binary classifier is the probability that the label is true (P(y)) and the probability that the label is false (1−P(y)). A threshold value of 0.5 is used to decide whether the predicted label is a 1 (P(y)>0.5) or a 0 (P(y)<=0.5). The decision to score a prediction as a 1 or a 0 depends on the threshold to which the predicted probabilities are compared. If we were to change the threshold, it would affect the predictions; and therefore change the metrics in the confusion matrix. A common way to evaluate a classifier is to examine the true positive rate (which is another name for recall) and the false positive rate (1 - specificity) for a range of possible thresholds. These rates are then plotted against all possible thresholds to form a chart known as a received operator characteristic (ROC) chart, like this:\n\n#tabulate probabilities\npredictions_glm <- predictions_glm %>% \n  bind_cols(logistic_glm %>% \n              predict(new_data = test_tbl, type = \"prob\"))\n\nWarning in predict.lm(object, newdata, se.fit, scale = 1, type = if (type == :\nprediction from a rank-deficient fit may be misleading\n\npredictions_rf <- predictions_rf %>% \n  bind_cols(rf_final %>% \n              predict(new_data = test_tbl, type = \"prob\"))\n\npredictions_svm <- predictions_svm %>% \n  bind_cols(svm_final %>% \n              predict(new_data = test_tbl, type = \"prob\"))\n\npredictions_xgb <- predictions_xgb %>% \n  bind_cols(xgb_final %>% \n              predict(new_data = test_tbl, type = \"prob\"))\n\n# Make a roc_chart\nroc_glm <- predictions_glm %>% \n  roc_curve(truth = undx_dm, .pred_No) %>% \n  autoplot() +\n  ggtitle(\"Logistic regression\")\n\nroc_rf <- predictions_rf %>% \n  roc_curve(truth = undx_dm, .pred_No) %>% \n  autoplot() +\n  ggtitle(\"Random forest\")\n\nroc_svm <- predictions_svm %>% \n  roc_curve(truth = undx_dm, .pred_No) %>% \n  autoplot() +\n  ggtitle(\"Supervised Vector Machines\")\n\n\nroc_xgb <- predictions_xgb %>% \n  roc_curve(truth = undx_dm, .pred_No) %>% \n  autoplot() +\n  ggtitle(\"xgBoost\")\n\n# put them together\nplot_grid(roc_glm, roc_rf, roc_svm, roc_xgb,nrow = 2)\n\n\n\n\nThe ROC chart shows the curve of the true and false positive rates for different threshold values between 0 and 1. A perfect classifier would have a curve that goes straight up the left side and straight across the top. The diagonal line across the chart represents the probability of predicting correctly with a 50/50 random prediction; so you obviously want the curve to be higher than that (or your model is no better than simply guessing!).\nThe area under the curve (AUC) is a value between 0 and 1 that quantifies the overall performance of the model. One way of interpreting AUC is as the probability that the model ranks a random positive example more highly than a random negative example. The closer to 1 this value is, the better the model. Once again, Tidymodels includes a function to calculate this metric: yardstick::roc_auc()\n\n\nMoving beyond: Explainable ML\nThere are several definition of interpretability in the context of a Machine Learning model. I vibe with “trust”. Trust that the model is predicting a certain value for the “right reasons”. These can by in no means replace the “odds ratio” for instance but have a different objective in mind.\n\n\n\nImportance of Explaining your Model (Source: Moneda, 2021)\n\n\nSHAP stands for Shapley Additive Explanations. It is model agnostic, efficient algorithm to compute feature importance.\n\n\n\nShapley equation (Source: Linberry, 2021)\n\n\nFor the purposes of this tutorial we shall only explore the use of the final xgboost model but the approach is transferable. First lets create a shapley dataset (resampled data).\n\n#define a recipe\nshap_recipe1 <- recipe(undx_dm~., data = train_tbl) %>%\n  step_novel(all_predictors(), -all_numeric()) %>% \n  step_string2factor(all_nominal(), -all_outcomes()) %>%\n  step_integer(sex) %>%\n  step_integer(residential) %>%\n  step_dummy(income, one_hot = T) %>%\n  step_dummy(smoke, one_hot = T) %>%\n  step_impute_linear(\n    bmi, mean_f, mean_v, vig_day, vig_min, rbs, rchol,\n    impute_with = imp_vars(sex, age)) %>%\n  step_impute_mean(smoke_Never, smoke_Former, smoke_Current) %>%\n  step_zv(all_predictors()) \n\n#rerun the final model with the new data\n#redefine a workflow\nshap_workflow<- workflow() %>% \n  add_recipe(shap_recipe1) %>% \n  add_model(xgb_cv)\n\n#run the model\nshap_final <- shap_workflow %>% \n  fit(train_tbl)\n\nWarning: There are new levels in a factor: NA\n\n#get a resampled shap dataframe\nshap_df <- bake(\n  prep(shap_recipe1),\n  new_data = train_tbl\n)\n\nWarning: There are new levels in a factor: NA\nThere are new levels in a factor: NA\n\n#setup a second recipe\nshap_recipe2 <- recipe(undx_dm~., data = shap_df) %>% \n  step_integer(all_nominal())\n\n#sample a 1000 rows\nshap_sample_df <- shap_df[sample(nrow(shap_df), 1000),]\n\n#create a matrix for the shap process\nshap_df_prep <- bake(\n  prep(shap_recipe2), \n  has_role(\"predictor\"),\n  new_data = shap_sample_df, \n  composition = \"matrix\"\n)\n\n#create a shap object\nshap <- shapviz(extract_fit_engine(shap_final), X_pred = shap_df_prep, X = shap_sample_df)\n\nThe shap tutuorial here utilises the shapviz package. These can produce quite impressive plots but they have their limitations (and certainly visually the Python shap package produces more and better plots). There are several important plots that we can develop using the shapley approach. This includes:\nSummary Plot (or Importance plot): The summary plot is a bar chart that shows the top features in descending order of importance. The features are colored according to their value, where blue indicates low values and red indicates high values. The height of the bar represents the impact of the feature on the model’s output. The further the bar is to the right, the higher the impact of the feature.\n\nsv_importance(shap, kind = \"beeswarm\", show_numbers = TRUE)\n\n\n\n\nDependence Plot: The dependence plot shows the relationship between a single feature and the predicted outcome of the model. The x-axis represents the feature’s values, and the y-axis represents the SHAP values, which represent the impact of the feature on the model’s output. The color of each point represents the value of another feature that is correlated with the feature in question. A steep upward slope suggests a positive correlation between the feature and the model’s output, while a steep downward slope suggests a negative correlation.\n\nsv_dependence(shap, \"rbs\", color_var = \"auto\")\n\n\n\nsv_dependence(shap, \"sys\", color_var = \"auto\")\n\n\n\n\nForce Plot: The force plot is a visual representation of how the input features contribute to the model’s output for a specific instance. It displays the SHAP value of each feature as a vertical bar, which can be either positive or negative. The horizontal position of the bar represents the magnitude of the SHAP value, and the color indicates whether the feature value is high or low. The force plot can help identify which features are contributing positively or negatively to the model’s output for a given instance.\n\nsv_force(shap)\n\n\n\n\nInteraction Plot: The interaction plot displays how the effect of one feature on the model’s output changes based on the value of another feature. Each point on the plot represents a specific instance, and the color represents the value of a third feature that is not included in the plot. A strong positive interaction suggests that the effect of one feature is amplified when the other feature is also present, while a strong negative interaction suggests that the effect of one feature is reduced when the other feature is present.\n\nsv_waterfall(shap, row_id = 1)"
  },
  {
    "objectID": "index.html#take-home-message",
    "href": "index.html#take-home-message",
    "title": "An Introduction to Machine Learning in R",
    "section": "Take home message",
    "text": "Take home message\n\nTry to continue exploring the recipes and workflow. Homework- try the functions\n\nrecipes: step_impute_**** (mean, knn, median etc)\nworkflows: add_formula(), update_formula(), fit_data(), fit_split(), update_model(), collect_predictions()\ndifferent shap packages- fastshap\n\nThis document is currently meant to serve as an introduction to workflows within the tidymodels ecosystem for machine learning R.\nFor use only within SBDR- Materials for INTERNAL USE ONLY\n\n\n\n\nJourney with R (AI Generated Image by DALL-E/ Vivek Jason)"
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "An Introduction to Machine Learning in R",
    "section": "References",
    "text": "References\n\nTidy Modelling with R\nTidymodelling in R Book Club\nTidymodels website\nThe Julia Silge Blog\nStatQuest"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  }
]